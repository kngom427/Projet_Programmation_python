Id	Texte	Sources
1	Hey everyone,  I‚Äôm planning to spend the next 2‚Äì3 months fully focused on Machine Learning. I already know Python, NumPy, Pandas, Matplotlib, Plotly, and the math side (linear algebra, probability, calculus basics), so I‚Äôm not starting from zero. The only part I really want to dive into now is Machine Learning itself.  What I‚Äôm looking for are resources that go deep and clear all concepts properly ‚Äî not just a surface-level intro. Something that makes sure I don‚Äôt miss anything important, from supervised/unsupervised learning to neural networks, optimization, and practical applications.  Could you suggest:  Courses / books / YouTube playlists that explain concepts thoroughly.  Practice resources / project ideas to actually apply what I learn.  Any structured study plan or roadmap you personally found effective.   Basically, if you had to master ML in 2‚Äì3 months with full dedication, what resources would you rely on?  Thanks a lot üôè 	reddit
2	I tried to make a simple code of model that predicts a possible price of laptop (https://www.kaggle.com/datasets/owm4096/laptop-prices/data) and then to evaluate accuracy of model's predictions, but I was confused that my accuracy did not increase after adding more columns of data (I began with 2 columns 'Ram' and 'Inches', and then I added more columns, but accuracy remained at 60 percent). I don't know all types of models of machine learning, but I want to somehow raise accuracy of predictions	reddit
3	Courses I found for learning ML  ->  Andrew ng (standford) -> [https://youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&si=CiL2kV6wgspPkphX](https://youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&si=CiL2kV6wgspPkphX) )  Andrew ng (deeplearning.ai) -> [https://youtube.com/playlist?list=PLkDaE6sCZn6FNC6YRfRQc\_FbeQrF8BwGI&si=tsLpAeVImHuMwQcR](https://youtube.com/playlist?list=PLkDaE6sCZn6FNC6YRfRQc_FbeQrF8BwGI&si=tsLpAeVImHuMwQcR)  Amazon ML school -> [https://youtube.com/playlist?list=PLBSzU4t3A-UURwuwY1cMoP4AXe66NAUMQ&si=F2FQsssfINqpd6CK](https://youtube.com/playlist?list=PLBSzU4t3A-UURwuwY1cMoP4AXe66NAUMQ&si=F2FQsssfINqpd6CK) )  Josh stammer -> [https://youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF\_7q2GfuJF&si=xaD-7NDzP8URzS9r](https://youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&si=xaD-7NDzP8URzS9r) )  3Blue1Brown -> [https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1\_67000Dx\_ZCJB-3pi&si=PUQx2976\_KvQFrbJ](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=PUQx2976_KvQFrbJ) )  freecodecamp -> [https://youtube.com/playlist?list=PLWKjhJtqVAblStefaz\_YOVpDWqcRScc2s&si=XDwUoKkZOEqNH1fy](https://youtube.com/playlist?list=PLWKjhJtqVAblStefaz_YOVpDWqcRScc2s&si=XDwUoKkZOEqNH1fy) )     I need suggestion which is better as in terms of concept and theory and how I should start learning ML if there are any other course that I have not mentioned here and that one is better then this do suggest it.  Also If anyone know ML concept That I should implement from scratch in code that show my understanding of the concept do suggest them.  Suggest some good research paper for learning or understanding ML and as well as implementing from scratch.	reddit
4	Hey everyone, I‚Äôve recently decided I want to learn Machine Learning üß†, but I don‚Äôt know much about Python yet (I only have some very basic programming knowledge).  I‚Äôm a bit confused about how to start:  Should I first focus on learning Python well before touching ML?  Or should I jump straight into an ML course and learn Python as I go?  Is it better to start with a project or complete a beginner-friendly course first?   Also, if anyone has recommendations for good beginner-friendly ML courses, especially ones that explain concepts in simple words and maybe have hands-on projects, please share! I‚Äôve heard about freeCodeCamp and Coursera‚Äôs Andrew Ng course, but not sure which is better for someone like me.  Any tips, resources, or step-by-step paths would be super helpful üôè.  Thanks in advance!	reddit
5	This feels relevant considering everyone's outspokenness on generative AI, machine learning, and the overall shitification of creatives. It's highly probable that Vimeo will start using their users content for such considering it's what Bending Spoons did with WeTransfer already.   I knew Vimeo's days are numbered but this sucks. You either die the (creative)hero or live long enough to see yourself become the (venture capitalist)villain. 	reddit
6	"It is omnipresent!  **First** of all, the peer-review process is *broken*. Every fourth NeurIPS submission is put on arXiv. There are DeepMind researchers publicly going after reviewers who are criticizing their ICLR submission. On top of that, papers by well-known institutes that were put on arXiv are accepted at top conferences, despite the reviewers agreeing on rejection. In contrast, vice versa, some papers with a majority of accepts are overruled by the AC. (I don't want to call any names, just have a look the openreview page of this year's ICRL).  **Secondly,** there is a *reproducibility crisis*. Tuning hyperparameters on the test set seem to be the standard practice nowadays. Papers that do not beat the current state-of-the-art method have a zero chance of getting accepted at a good conference. As a result, hyperparameters get tuned and subtle tricks implemented to observe a gain in performance where there isn't any.  **Thirdly,** there is a *worshiping* problem. Every paper with a Stanford or DeepMind affiliation gets praised like a breakthrough. For instance, BERT has seven times more citations than ULMfit. The Google affiliation gives so much credibility and visibility to a paper. At every ICML conference, there is a crowd of people in front of every DeepMind poster, regardless of the content of the work. The same story happened with the Zoom meetings at the virtual ICLR 2020. Moreover, NeurIPS 2020 had twice as many submissions as ICML, even though both are top-tier ML conferences. Why? Why is the name ""neural"" praised so much? Next, Bengio, Hinton, and LeCun are truly deep learning pioneers but calling them the ""godfathers"" of AI is insane. It has reached the level of a cult.  **Fourthly**, the way Yann LeCun talked about biases and fairness topics was insensitive. However, the *toxicity* and backlash that he received are beyond any reasonable quantity. Getting rid of LeCun and silencing people won't solve any issue.  **Fifthly**, machine learning, and computer science in general, have a huge *diversity problem*. At our CS faculty, only 30% of undergrads and 15% of the professors are women. Going on parental leave during a PhD or post-doc usually means the end of an academic career. However, this lack of diversity is often abused as an excuse to shield certain people from any form of criticism.  Reducing every negative comment in a scientific discussion to race and gender creates a toxic environment. People are becoming afraid to engage in fear of being called a racist or sexist, which in turn reinforces the diversity problem.  **Sixthly**, moral and ethics are set *arbitrarily*. The U.S. domestic politics dominate every discussion. At this very moment, thousands of Uyghurs are put into concentration camps based on computer vision algorithms invented by this community, and nobody seems even remotely to care. Adding a ""broader impact"" section at the end of every people will not make this stop. There are huge shitstorms because a researcher wasn't mentioned in an article. Meanwhile, the 1-billion+ people continent of Africa is virtually excluded from any meaningful ML discussion (besides a few Indaba workshops).  **Seventhly**, there is a cut-throat publish-or-perish *mentality*. If you don't publish 5+ NeurIPS/ICML papers per year, you are a looser. Research groups have become so large that the PI does not even know the name of every PhD student anymore. Certain people submit 50+ papers per year to NeurIPS. The sole purpose of writing a paper has become to having one more NeurIPS paper in your CV. Quality is secondary; passing the peer-preview stage has become the primary objective.  **Finally**, discussions have become *disrespectful*. Schmidhuber calls Hinton a thief, Gebru calls LeCun a white supremacist, Anandkumar calls Marcus a sexist, everybody is under attack, but nothing is improved.  Albert Einstein was opposing the theory of [quantum mechanics](https://en.wikipedia.org/wiki/Albert_Einstein#Einstein's_objections_to_quantum_mechanics). Can we please stop demonizing those who do not share our exact views. We are allowed to disagree without going for the jugular.   The moment we start silencing people because of their opinion is the moment scientific and societal progress dies.   Best intentions, Yusuf"	reddit
7	This post has a few graphs. If you don't want to click on each one individually, they're all in an imgur album [here](https://imgur.com/a/5nkFTu4).  There is a tl;dr at the end of the post.  ***  #Introduction  When picking in the top-10 of a draft, teams have one goal: select a franchise-altering player with star potential. Though some teams draft for need and prefer to select more NBA-ready players, in general, GMs do their best to select a player who may become a star.  This is very challenging. Many factors affect a player‚Äôs ability to become a star. Along with college performance, factors like athleticism, intangibles, injuries, coaching, and more change a player‚Äôs star potential.  As fans on the outside looking in, we have limited information on most of these factors except one: college performance. Though even the college performance of many players needs context (such as Cam Reddish‚Äôs low volume stats due to playing with Zion Williamson and R.J. Barrett), it‚Äôs one of the only quantifiable factors we can use. So, let‚Äôs try to use college stats to predict All-Stars in the top-10 of the 2019 draft.  ***  #Methods  First, I created a database of every top-10 pick from the 1990-2015 NBA drafts. We use 1990 as the limit because it ensures every player played their entire college career with a 3-point line. The 2015 draft was set as an upper limit so that all players played the entirety of their rookie contract, giving them some chance to make an All-Star team.  In addition to collecting their college stats, I marked whether the prospect made an All-Star team. There is no consideration for whether the player became an All-Star while on the team that drafted him, how long it took him to get there, etc. All data was collected from Sports-Reference.  Players who made an All-Star team at some point in their career earned a ‚Äú1‚Äù in the All-Star column. Meanwhile, players who failed to make an All-Star team earned a ‚Äú0.‚Äù  This represents a binary classification problem. There are two classes we‚Äôre looking at: All-Star and not All-Star. The models try to match each player to one of the two classes. We‚Äôll also look at the prediction probability (probability for the player to be in the class) the models give each player.  To create the models, we used the following stats as inputs:  |Counting stats|Efficiency|Other| :--|:--|:--| |PPG|TS%|Pick| |TRB|3PAr|SOS| |AST|FTr|| |STL||| |BLK|||  Note that win shares, box plus/minus, and other holistic advanced stats that are excluded. College BPM data is available only from the 2011 draft, and college WS data is available only from the 1996 draft. Therefore, using BPM restricts the data set massively. Though adding WS only excludes 6 years of drafts, the models were significantly less accurate when including WS.  The models predicted whether the player made an All-Star team (the 1s or 0s described above).  We collected the same set of stats for the top-10 picks in the 2019 draft. When using the models to All-Stars out of the 2019 draft, we‚Äôll look primarily at the prediction probabilities of the positive class. A prediction probability of 0.75 indicates that the model is 75% certain the player will fall into class 1 (All-Star). Therefore, every player with a prediction probability above 0.5 would be predicted as a 1 if we just used the models to predict classes instead of probability.  Given that about 31% of top-10 picks since 1990, the prediction probabilities give us more information about the predictions. If we‚Äôd just predict the classes, we‚Äôd likely get 2-4 1s, and the rest be 0s. However, with the prediction probabilities, we can see whether a player has a higher All-Star probability than others drafted at his pick historically, making him a seemingly good value.  Note that unlike other problems like predicting All-NBA teams ‚Äì where voters have general tendencies making the problem easy to predict accurately ‚Äì predicting All-Stars is incredibly difficult. Players develop differently, and college stats alone are not nearly enough to accurately project a player‚Äôs All-Star potential. We don‚Äôt expect the models to incredibly accurate. After all, if they were, teams would use better models higher quality data to make predictions that would help them always pick All-Stars.  In total, we made four models:  1. Logistic classifier (LOG) 2. Support vector classifier (SVC) 3. Random forest classifier (RF) 4. Gradient boosting classifier (GBC)  ***  #Comparing All-Star and not All-Star stats  Let‚Äôs compare some college stats between All-Stars and not All-Stars. This will illustrate just how difficult it is to differentiate the two groups based off just their college stats.  Before diving into the differences (or lack thereof), let‚Äôs first establish how to read these plots. This type of graph is called a boxplot. The yellow line represents the median or middle value in each group. The top of the box signifies the 75th percentile, while the bottom of the box signifies the 25th percentile. So, the 25th-50th percentile can be seen between the bottom of the box and the yellow line. From the yellow line to the top of the box represents the 50th-75th percentile. The full box represents the 25th-75th percentile of the data.  The lines flowing out of the box are called ‚Äúwhiskers.‚Äù The top of the whisker, or the ‚ÄúT‚Äù shape, represents the greatest value, excluding outliers. The bottom whisker represents the opposite (the lowest value excluding outliers). From the top of the box to the top of the whisker represents the 75th-100th percentile. The bottom of the box to the bottom of the whisker represents the 0th-25th percentile. Therefore, the top of the box also represents the median of the upper half of the data set.  The dots above or below the whiskers represent outliers. Outliers above the whiskers represent points that are greater than the upper quartile (top of the box) + 1.5 times then interquartile range (top of the box ‚Äì bottom of the box). Outliers below the whiskers represent points that are less than the lower quartile (bottom of the box) ‚Äì 1.5 times then interquartile range (top of the box ‚Äì bottom of the box).  First, let‚Äôs look at their points per game.  https://i.imgur.com/W344Rfe.png  Though the All-Stars have a marginally higher median PPG, the not All-Stars have a higher upper quartile PPG (top of the whisker). Therefore, there‚Äôs no clear difference here between the two groups, especially given that the bottom whiskers extend similarly for both groups.  Next, let‚Äôs look at rebounds and assists. Because big men will get more rebounds, and guards will get more assists, All-Stars and not All-Stars seems to be an odd comparison. However, we‚Äôre just looking for differences in basic counting stats.  https://i.imgur.com/P9vayUu.png  https://i.imgur.com/GoSlUqV.png  For rebounds, there‚Äôs practically no difference yet again. Both groups show a nearly identical median and very similar ranges. For assists, the All-Stars have a higher median assist total, and the 25th-75th percentile range stretches higher. Therefore, there‚Äôs a small difference between the two.  Let‚Äôs look at the difference in strength of schedule (SOS).  https://i.imgur.com/ejj28M6.png  Yet again, there‚Äôs a minimal difference. The medians are almost equal. Though the All-Stars range is higher than the not All-Stars range, there are multiple low outliers for the All-Stars.  Lastly, let‚Äôs look at the difference in picks.  https://i.imgur.com/D95LjtS.png  This is the first pronounced difference. The median pick of an All-Star is much lower than that of a not All-Star. Because no other stats showed any significant difference between the two groups, we can expect pick to be the most important feature in the models. Furthermore, this difference shows that NBA GMs are generally pretty good at drafting.   ***  #Model analysis  ##Model creation: data transformation  After creating the four models described above and testing their accuracy with basic metrics (discussed later), I did two things.  First, I tried manipulating the data. To make the models, I initially used the raw data. Sometimes, normalizing the data may lead to better performance. Normalizing the data means scaling each individual stat so that the highest value is 1 and the lowest value is 0. This can be done across the entire data set (the player with the highest college PPG would have a PPG input to the models of 1) or to each draft year (the player with the highest college PPG in each draft year would have a PPG input to the models of 1). Neither of these methods increased performance.  Next, I tried transforming the data into ranks. Instead of giving raw or normalized stats, we can simply rank all the players by their stats. Like normalization, this gives us some method to compare the players. However, ranking each stat for neither the entire data set nor each draft year improved performance.  After all, we‚Äôll use the usual, raw data we got from Sports Reference.  ##Model creation: hyperparameter tuning  Every model has certain characteristics that determine how the model fits the data. These characteristics, or hyperparameters, make the model‚Äôs architecture. For example, if we were using an exponential model, the degree (quadratic, cubic, quartic, etc.) would be a hyperparameter. Hyperparameters impact the model‚Äôs performance.  In previous posts, I used nice round numbers for the model hyperparameters and played around with them randomly until I found a mix that yielded a strong model. However, this is not scientific.  For a scientific hyperparameter tuning, we can use a method called grid search. Grid search takes a grid of possible values for hyperparameters we want to test, creates a model for each possible combination, evaluates the model‚Äôs accuracy, and returns the ‚Äúbest‚Äù model. In this case, we want to find the model that has the best recall (a metric we‚Äôll discuss soon).  The SVC, RF, and GBC saw their performance improve with the hyperparameters from the grid search. So, for those models, we used the best parameters found by the grid search. For the LOG, we used the parameters we set before the grid search (in this case, the default).  ##Basic goodness-of-fit  We measure the performance of classification models in several ways. The simplest metric is accuracy, which measures the percentage of predictions the model made correctly. Essentially, it takes the list of predictions and finds how many values in the list were perfect matches to the list of results.  Because this is the simplest classification metric, it has its flaws. Accuracy only measures correct predictions, so it may be misleading in some cases. For example, if we‚Äôre predicting something very rare, then almost all the results will be 0s. Therefore, a model that exclusively predicts 0s will have a high accuracy even if it has no predictive power.  Given that there are more not All-Stars than All-Stars, accuracy is not the best metric in this case. 30% of the testing set consists of All-Stars, meaning a model could achieve 70% accuracy by predicting all 0s (that no one will be an All-Star). However, because picking correct All-Stars at the expense of picking some incorrect All-Stars is better than picking no All-Stars at all, it‚Äôs fine to have an accuracy less than 70%.  To understand the next few classification metrics, we must first establish some terms. A true positive occurs when the model predicts a 1, and the actual value is a 1 (meaning the model correctly predicted an All-Star). A true negative is the opposite; the model correctly predicts a 0. False positives occur when the model predicts a 1 where the actual value is 0, and false negatives occur when the model predicts a 0 where the actual value is 1.  Recall measures a model‚Äôs ability to predict the positive class. In this case, it‚Äôs the model‚Äôs ability to find all the All-Stars (true positives). Recall = TP / (TP + FN), meaning that a ‚Äúperfect‚Äù model that predicts every positive class correctly will have a recall of 1. Recall is arguably the most important metric here.  Precision measures how many of the returned predicted All-Stars were true. It penalizes the model for incorrectly predicting a bunch of All-Stars. Precision = TP / (TP + FP), meaning that a ‚Äúperfect‚Äù model will have a precision of 1. Notice that there is typically a trade-off between precision and recall given that recall measures ability to find true positives, while precision measures ability to limit false positives.  To combine the two metrics, we can use F1. F1 = 2(precision * recall) / (precision + recall). By combining precision and recall, F1 lets us compare two models with different precisions and recalls. Like recall and precision, F1 values are between 0 and 1, with 1 being the best.  Now that we‚Äôre familiar with some classification metrics, let‚Äôs examine the models‚Äô performance. The table below shows the scores of all four models on the previously mentioned metrics.  |Model|Accuracy|Recall|Precision|F1| :--|--:|--:|--:|--:| |LOG|0.746|0.316|0.667|0.429| |SVC|0.762|0.263|0.833|0.4| |RF|0.746|0.368|0.636|0.467| |GBC|0.73|0.368|0.583|0.452|  The RF and GBC had the highest recall, though the RF had higher precision and accuracy than the GBC. Although the SVC had the highest precision and accuracy, we‚Äôre most concerned with recall, meaning the other models are stronger. The LOG appears slightly weaker than the RF and GBC, though it‚Äôs still a strong model.  As mentioned before, we‚Äôre not expecting dazzling performance from the models. After all, if models using publicly available data could predict All-Stars, NBA teams with full analytics staffs would have no problem finding them. Therefore, though these metrics are not encouraging by themselves, they show that the models have some predictive power.  ##Improvement over random  To show that the models are stronger than randomly predicting All-Stars, I made a dummy classifier. The dummy classifier randomly predicts players to be a 1 or 0 with respect to the training set‚Äôs class distribution. Given that the training set had 32% All-Stars (the testing set had 30% as mentioned earlier), the dummy classifier will randomly predict 32% of the testing set to be All-Stars.  The table below shows the dummy classifier‚Äôs performance.  |Model|Accuracy|Recall|Precision|F1| :--|--:|--:|--:|--:| |Dummy|0.556|0.316|0.286|0.3|  Each of our four models has higher accuracy, precision, and F1 scores than the dummy classifier. It is slightly concerning that the dummy classifier has equal recall to the LOG and higher recall than the SVC. Nevertheless, the LOG and SVC were much better at getting their All-Star predictions correct when they did predict them (higher precision).  ##Confusion matrices  To help visualize a model‚Äôs accuracy, we can use a confusion matrix. A confusion matrix shows the predicted vs. actual classes in the test set for each model. It plots each model‚Äôs true positives (bottom right), true negatives (top left), false positive (top right), and false negatives (bottom left) in a square.  The testing set was small; it had only 63 data points. Below are the confusion matrices for all four models.  https://i.imgur.com/H1DeMjc.png  https://i.imgur.com/kTgdOrV.png  https://i.imgur.com/jgQmTDV.png  https://i.imgur.com/NjcmZW9.png  ##Cross-validation  As we do in other machine learning posts, we want to cross-validate our models. This will ensure that they didn‚Äôt ‚Äúmemorize‚Äù the correct weights for this specific split of data, meaning they overfit.  In classification problems, it‚Äôs important to see that the class balance is close to even between the training and testing set. This could influence cross-validation, given that a different split of the data might have a large imbalance. Our training set had 32% All-Stars while our testing set had 30% All-Stars, making this a non-factor.  The table below shows the cross-validated accuracy (k = 3) and the scores‚Äô 95% confidence interval.  |Model|CV accuracy|95% confidence interval| :--|--:|:--| |LOG|0.665|+/- 0.096| |SVC|0.683|+/- 0.027| |RF|0.746|+/- 0.136| |GBC|0.633|+/- 0.135|  Every model has a cross-validated accuracy score that‚Äôs close to its real accuracy score.  ##Log loss and ROC curves  The final metrics we‚Äôll use are log loss and ROC curves.  Log loss is essentially like accuracy with prediction probabilities instead of predicted classes. Lower log loss is better. Because we‚Äôre interested in the prediction probabilities, log loss is an important metric here. Though log loss isn‚Äôt exactly simple to interpret by itself, it‚Äôs useful for comparing models.  The table below shows the four models‚Äô log loss values.  |Model|Log loss| :--|--:| |LOG|0.546| |SVC|0.56| |RF|0.556| |GBC|1.028|  The biggest takeaway from the log loss is that the GBC may not be as strong as we initially thought, given that all the other models have significantly lower log loss scores.  The second to last metric we‚Äôll look at is the receiver operating characteristics (ROC) curve and the area under it. The curve shows the ‚Äúseparation‚Äù between true positives and true negatives by plotting them against each other. The area gives us a numerical value for this separation.  A model with no overlap in probability between TP and TN (perfect) would have a right-angled ROC curve and an area under the curve of 1. As the overlap increases (meaning the model is worse) the curve gets closer to the line y = x.  The ROC curves and the area under the curve for each model is below.  https://i.imgur.com/kmGla77.png  Each model has a similar ROC curve and area under the curve.  ***  #Why do the models predict what they do?  Before going into the results, the last thing we‚Äôll want to look at is what the models find important in predicting All-Stars. There are a couple ways to do this.  First, we‚Äôll look at the model coefficients and feature importances. The LOG and SVC have coefficients, while the RF and GBC have feature importances. Coefficients are different from feature importances in that the coefficients are used to express the model in an equation. Higher coefficients do not mean the feature is more important, they just mean the model scaled that feature differently. On their own, they don‚Äôt have much meaning for us, but for comparison purposes, we can see which model scales a certain factor more.  The graph below shows the coefficients of the LOG and SVC.  https://i.imgur.com/MjISg1X.png  The two models have very similar coefficients for the most part. The two main differences are in the steals and blocks coefficients. While the LOG gives blocks a negative coefficient, the SVC gives it a positive coefficient. Furthermore, the LOG gives steals a much higher coefficient than the SVC.  Next, let‚Äôs look at feature importances. Feature importance shows how much the model relies on a feature by measuring how much the model‚Äôs error increases without it. Higher feature importance indicates more reliance on the feature.  The graph below shows the feature importances of the RF and GBC.  https://i.imgur.com/mNUa0SW.png  As we would expect, pick was the most important feature for both models (the GBC point covers the RF point). Interestingly, SOS was almost as important to the GBC as pick.  ##Shapley values  To get a more detailed view of how each feature impacted each model, we can use a more advanced model explanation metric called Shapley values.  Shapley value is defined as the ‚Äúaverage marginal contribution of a feature value over all possible coalitions.‚Äù It tests every prediction for an instance using every combo of our inputs. This along with other similar methods gives us more information about how much each individual feature affects each model in each case.  First, we‚Äôll look at the mean SHAP value, or average impact of each feature on each of the four models. A higher value indicates a more important feature.  The four graphs below show the mean SHAP values for each of the four models (in order of LOG, SVC, RF, GBC).  https://i.imgur.com/2zv7BGd.png  https://i.imgur.com/ysMmlhg.png  https://i.imgur.com/GqRoVj7.png  https://i.imgur.com/51GcrlK.png  The LOG, RF, and GBC all have pick as the most important feature, as expected. Steals being the second most important feature is surprising. The three models all have pick, steals, rebounds, and assists in their top-5 most important features.  The SVC has odd results, as pick was only the third most important feature behind rebounds and assists.  To get a more detailed and individualized view of the feature impacts, we can look at the SHAP value for each point.  In the graphs below, the x-axis represents the SHAP value. The higher the magnitude on the x-axis (very positive or very negative), the more the feature impacts the model. The color indicates the feature value, with red being high values and blue being low values. So, a blue point for pick indicates the player was picked early.  With these plots, we can make conclusions like ‚Äúpick is very important to the models when its value is low but becomes less important as players are picked later.‚Äù  The four graphs below show the individual point SHAP and feature values.  https://i.imgur.com/FbarVSw.png  https://i.imgur.com/HKheCGM.png  https://i.imgur.com/CUSmVbd.png  https://i.imgur.com/puJObd8.png  For the LOG, pick mattered a lot when its value was low. As players were picked later, it had less of an impact on model output. The SVC was more affected by high assists, rebounds, and steal values than low pick values, unlike other models.  Rebounds had minimal impact on the RF except for cases where the player‚Äôs rebound total was very low. The opposite is true for TS% in both the RF and GBC; generally, TS% had minimal impact on the model except for the highest TS% values. For the GBC, the highest SOS values had a very high impact on model output.  ***  #Results  To make predictions for the 2019 draft, we looked at prediction probabilities instead of predicted classes. This gives us each model‚Äôs probability that the player makes an All-Star team.  The four graphs below show each model‚Äôs predictions.  https://i.imgur.com/RohPa4F.png  https://i.imgur.com/mIlxG9X.png  https://i.imgur.com/HqmnVoc.png  https://i.imgur.com/9wKvXAY.png  Every model gives Zion the highest All-Star probability. The LOG and SVC‚Äôs top-3 in All-Star probability mimic the draft‚Äôs top-3. However, the RF and GBC love Jaxson Hayes; both models gave him the second-highest All-Star probability, just above Ja Morant. Both the RF and GBC also dislike DeAndre Hunter, giving him the lowest All-Star probability.  The graph below shows the average prediction of the four models.  https://i.imgur.com/c9JSRWj.png  The RF and GBC propel Jaxson Hayes to the fourth-highest average predicted All-Star probability.  The table below shows each model's predictions and the average of the predictions.  |Pick|Player|LOG|SVC|RF|GBC|Average| --:|:--|--:|--:|--:|--:|--:| |1|Zion Williamson|0.71|0.63|0.80|1.00|0.78| |2|Ja Morant|0.65|0.49|0.58|0.91|0.66| |3|RJ Barrett|0.37|0.49|0.53|0.62|0.50| |4|DeAndre Hunter|0.22|0.23|0.16|0.00|0.15| |5|Darius Garland|0.19|0.24|0.42|0.10|0.23| |6|Jarrett Culver|0.25|0.30|0.48|0.47|0.37| |7|Coby White|0.15|0.27|0.31|0.16|0.22| |8|Jaxson Hayes|0.08|0.17|0.61|0.94|0.45| |9|Rui Hachimura|0.07|0.11|0.17|0.00|0.09| |10|Cam Reddish|0.10|0.20|0.35|0.28|0.23|  To determine the best value picks according to the models, we can compare each player‚Äôs predicted All-Star probability to the percent of players drafted in his slot that made an All-Star team in our data set (1990-2015 drafts). So, if a first pick and a tenth pick both have 80% All-Star probability, the tenth pick will be a better relative value because many more first picks make All-Star teams.  The graph below shows the All-Star probability minus the percent of players drafted in the slot that make an All-Star team for each player.  https://i.imgur.com/Akivph3.png  The graph below sorts the difference from greatest to least.  https://i.imgur.com/IySAp4R.png  The models love Ja Morant and Jaxson Hayes as great values. Meanwhile, the models dislike the #4 and #5 picks ‚Äì DeAndre Hunter and Darius Garland.  Part of the reason Morant has such a large difference is that #2 picks have an unusually low All-Star total. The table below shows the difference in All-Star probability. Notice that only 40% of #2 picks in our data set made an All-Star team, while 56% of #3 picks made one.  |Pick|Player|All-Star % at pick # since 1990|Average prediction|Difference| --:|:--|--:|--:|--:| |1|Zion Williamson|0.64|0.78|0.14| |2|Ja Morant|0.4|0.66|0.26| |3|RJ Barrett|0.56|0.50|-0.06| |4|DeAndre Hunter|0.32|0.15|-0.17| |5|Darius Garland|0.4|0.23|-0.17| |6|Jarrett Culver|0.24|0.37|0.13| |7|Coby White|0.08|0.22|0.14| |8|Jaxson Hayes|0.2|0.45|0.25| |9|Rui Hachimura|0.16|0.09|-0.07| |10|Cam Reddish|0.12|0.23|0.11|  ***  #Conclusion  Because predicting All-Stars is difficult and depends on more than just college stats, our models are not objectively accurate. Nevertheless, they can provide insight into the All-Star probabilities of the top-10 picks of this year‚Äôs draft.  Each of the four models predicts Zion is the most likely player to make an All-Star team. Two of the models give the second spot to Morant, while two of the models give the spot to Jaxson Hayes. Relative to historical All-Stars drafted at each slot, Morant and Hayes appear to be great values, while Hunter and Garland appear poor values.  ***  **TL;DR**: [Average predictions graph](https://i.imgur.com/c9JSRWj.png), [value above average All-Star percentage graph](https://i.imgur.com/IySAp4R.png). To see the individual values of these graphs, look at the two tables above.  ***  This is my newest post on my open-source basketball analytics blog, [Dribble Analytics](https://dribbleanalytics.blog).  The GitHub for the this project is [here](https://github.com/dribbleanalytics/2019-draft-ml).  You can check out the original piece [here](https://dribbleanalytics.blog/2019/07/2019-draft-ml-all-stars/).	reddit
8	Hello there, r/MachineLearning,  Recently, Reddit has announced some [changes to their API](https://www.reddit.com/r/modnews/comments/13wshdp/api_update_continued_access_to_our_api_for/) that may have pretty serious impact on many of it's users.  [You may have already seen quite a few posts like these](https://www.reddit.com/r/ModCoord/comments/1401qw5/incomplete_and_growing_list_of_participating/) across some of the other subreddits that you browse, so we're just going to cut to the chase.  # What's Happening  Third Party Reddit apps (such as Apollo, Reddit is Fun and others) are going to become ludicrously more expensive for it's developers to run, which will in turn either kill the apps, or result in a monthly fee to the users if they choose to use one of those apps to browse. Put simply, each request to Reddit within these mobile apps will cost the developer money. The developers of Apollo [were quoted around $2 million per month](https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/) for the current rate of usage. The only way for these apps to continue to be viable for the developer is if you (the user) pay a monthly fee, and realistically, this is most likely going to just outright kill them. **Put simply: If you use a third party app to browse Reddit, you will most likely no longer be able to do so, or be charged a monthly fee to keep it viable.**  In lieu of what's happening, [an open letter](https://www.reddit.com/r/ModCoord/comments/13xh1e7/an_open_letter_on_the_state_of_affairs_regarding/) has been released by the broader moderation community. Part of this initiative includes a potential subreddit blackout (meaning, the subreddit will be privatized) on June 12th, lasting 24-48 hours or longer. On one hand, this is great to hopefully make enough of an impact to influence Reddit to change their minds on this. On the other hand, we usually stay out of these blackouts, and we would rather not negatively impact usage of the subreddit.  We would like to give the community a voice in this. Is this an important enough matter that r/machinelearning should fully support the protest and blackout the subreddit on June 12th? Feel free to leave your thoughts and opinions below.   Also, please use up/downvotes for this submission to make yourself heard: upvote: r/ML should join the protest, downvote: r/ML should not join the protest.	reddit
9	A lot of people (like me) dive  into ML thinking it's about understanding intelligence, learning, or even just clever math ‚Äî and then they wake up buried under a pile of frameworks, configs, random seeds, hyperparameter grids, and Google Colab crashes. And the worst part? No one tells you how undefined the field really is until you're *knee-deep in the swamp.*  In mathematics:  * There's structure. Rigor. A kind of calm beauty in clarity. * You can prove something and *know* it‚Äôs true. * You explore the unknown, yes ‚Äî but on solid ground.  In ML:  * You fumble through a foggy mess of tunable knobs and lucky guesses. * ‚ÄúReproducibility‚Äù is a fantasy. * Half the field is just ‚Äúwhat worked better for us‚Äù and the other half is trying to explain it *after* the fact. * Nobody *really* knows why half of it works, and yet they act like they do.	reddit
10	So I had the opportunity to use a pre-trained StyleGan model for free and used a folder of 1000+ league skins (up to KDA all out) to see if I could make something cool.  Most of them don't resemble any humans so good luck trying to decipher what any of these are supposed to be (maybe new void champ?), but I guess it would help to also know these were trained off a model used for sceneries and landscapes of mountains, so yeah probably not the most accurate model to use.  If you wanna check out the full album they're [here](https://imgur.com/a/eMdwaDb)  &#x200B;  https://preview.redd.it/gs0byrdo2cy51.png?width=1280&format=png&auto=webp&s=9eed9a36bc5b4b0549e7fecc0e2da58e87bea9c8  &#x200B;  https://preview.redd.it/p8mgl1333cy51.png?width=1280&format=png&auto=webp&s=bba5b42242fcf7340ae053957ed4c4ae18061fe7	reddit
11	This post has a lot of graphs. If you don't want to click on each one individually, they're all in an imgur album [here](https://imgur.com/a/7SG7dQ4).  There is a tl;dr at the very end.  ***  # Introduction  With James Harden's recent stretch, the Nuggets' success despite their injuries, and the Bucks maintaining the second seed in the NBA, Harden, Jokic, and Giannis all have strong cases for MVP this year. Though Harden hasn't had the same team success as Jokic and Giannis, he's been averaging an absurd 34.2 PPG - almost 5 PPG more than second place Steph Curry. On the other hand, Jokic has been leading an injury-riddled Nuggets team to the first seed in the West. Along the way, he's had numerous incredible all-around performances, such as Monday's 40/10/8 game against the Trail Blazers. Jokic averages the 9th most assists per game in the league, an unprecedented figure for a center. Meanwhile, Giannis has been posting a hyper-efficient 27/12/6 while leading the Bucks to the NBA's second best record. In addition to these players, Curry and Kawhi have legitimate arguments for MVP, as they've played very well for the best teams in the league.  For much of the beginning of the season, many saw Giannis as the MVP favorite. However, the race seems tighter now than ever.  To predict who will win MVP now that we're just past the season's midway point, I created various models that try to predict this year's MVP.  ***  #Methods  This post uses very similar methods to [my post that analyzed the least and most deserving MVPs of the past decade](https://dribbleanalytics.blogspot.com/2018/08/least-most-deserving-mvps.html). Like the initial MVP post, I use a database of all players who placed top-10 in MVP voting since the 1979-1980 season (when the 3-point line was introduced). I measured the following stats for these players:  | Counting stats | Advanced stats| MVP votes | Team stats | | ------------- | ------------- | ------------- | -------- | | G | WS | MVP votes won | Wins | | MPG | WS/48 | Maximum MVP votes | Overall seed | | PTS/G | VORP | Share of MVP votes* |  | | TRB/G | BPM | | | AST/G | | | | STL/G | | | | BLK/G | | | | FG% | | | | 3P% | | | | FT% | | |  *Share of MVP votes = % of maximum votes received (so Curry's unanimous MVP is a vote share of 1).  For both this season and lockout seasons, I scaled up win shares, VORP, and wins to an 82 game season.  My initial MVP post used many of the above stats to predict vote share. The new models used in this post use fewer parameters, as several of the parameters in the previous post introduced collinearity. For example, having VORP, BPM, and MPG together is bad, as VORP adjusts BPM to MPG. For these models, I used only the following stats as parameters:  | Counting stats | Advanced stats | Team stats | | ------------- | -------------- | ---------- | | PTS/G | VORP | Wins | | TRB/G | WS | Overall seed | | AST/G | | | | FG% | | |  Using these parameters to predict vote share, I created 4 models:  1. Support vector regression (SVM) 2. Random forest regression (RF) 3. k-nearest neighbors regression (KNN) 4. Deep neural network (DNN)  Unlike the previous MVP post, the models were trained and tested using all data from the 1979-1980 season up until the 2017-2018 season. They then predict the MVP from this year's data. I predicted the MVP vote share for the top-10 players on NBA.com's January 11th MVP ladder.  ***  #Where the models fall short  Like we discussed in the previous MVP post, the MVP isn't determined only by who has the best stats. Because the models only know stats, they can't account for other factors that a media member considers when voting for MVP. These include:  - Narrative - Expectations; the models don't know that the Nuggets weren't expected to be this good - Popularity; the models don't know that Giannis and Harden are much more popular players than Jokic - How someone scores their points; the models don't know that Harden shoots a lot of free throws - Triple doubles and other arbitrary thresholds  Given that the models can't account for these factors, one way we can think about these predictions is that I'm evaluating who has the most MVP-like stats out of the players.  ***  #Understanding the data  The models predict the vote share for each of the players who were top-10 in MVP votes. It's important to note that most players will have a very low vote share, as there's usually only a few legitimate MVP contenders; the rest usually get some last place votes. To understand this distribution, I plotted a histogram of the vote share (see below).  https://i.imgur.com/4KmOOgZ.png  As expected, the distribution is not normal. Most of the players had a vote share less than 0.2. This is important to consider, as a skewed dataset like this introduces more variance to the models.  ***  #Regression analysis  ## Basic goodness-of-fit and cross-validation  The table below shows the r-squared and mean squared error for each of the four models. A higher r-squared and lower mean squared error indicate a more accurate model.  | Model | r-squared | Mean squared error | | ----- | --------- | ------------------ | | SVM | 0.708 | 0.028 | | RF | 0.628 | 0.035 | | KNN | 0.633 | 0.035 | | DNN | 0.619 | 0.036 |  As with the previous MVP post, the models don't have a very high r-squared. For the most part, the models have a higher r-squared and lower mean squared error than the previous post's models, as they had more data. Though the r-squared isn't great, the mean squared is very low. In most MVP races, the winner had a vote share advantage over second place above 0.1. Therefore, because of the low mean squared error compared to the actual vote share difference, we can expect the models to be accurate in predicting the MVP.  To ensure the models aren't overfitting - or just learning the correct coefficients that yield the best results given the specific split - I performed k-fold cross-validation for r-squared. Ideally, the cross-validated r-squared will be close to the r-squared mentioned above. This would mean the models perform equally well when tested on a different split of the data. The table below shows the cross-validated r-squared scores and their 95% confidence intervals.  | Model | CV r-squared | 95% confidence interval | | ----- | ------------ | ----------------------- | | SVM | 0.63 | +/- 0.10 | | RF | 0.50 | +/- 0.17 | | KNN | 0.53 | +/- 0.15 | | DNN | 0.57 | +/- 0.02 |  All the models have a cross-validated r-squared that's decently lower than the initial r-squared. Furthermore, the confidence intervals are wide. Therefore, it's possible the models are overfitting and may perform slightly worse on different data. However, even with these low r-squared values, the mean squared error on different datasets will not change drastically - it'll still be smaller than the usual difference between the MVP winner and second place.  ##Standardized residuals test  A standardized residuals test analyzes the difference between the model's predicted and actual values on historical data. To pass the test, the models have at least 95% of their standardized residuals within 2 standard deviations of the mean and have no noticeable trend. Passing the standardized residuals test gives us a first indication that the models' errors are random.  The graph below shows the standardized residuals of all four models.  https://i.imgur.com/GIpOQ9u.png  Only the RF fails the standardized residuals test. Furthermore, there is no noticeable trend. The only concerning result from the test is the high number of values greater than 2 and below -2. This indicates high variance in the models. However, it's important to note the skewed data likely contributes to this variance.  ##Q-Q plot  While the standardized residuals test determines if the errors are random, a Q-Q (quantile-quantile) plot examines if the errors are normally distributed. Normally distributed errors are important because they indicate that the model will perform similarly across the entirety of a dataset. In making a Q-Q plot, we're looking for the models' residuals to be as close to a 45-degree (y = x) line as possible - that indicates a normal distribution.  The graph below is the Q-Q plot for all four models' residuals.  https://i.imgur.com/UZ1n8CH.png  Though all four models appear to have residuals pretty close to the red y = x line, there are some considerable jumps at the upper quantiles of the residuals. Each models' residuals appear to be slightly bow-shaped; they rise above the red line at the lower quantiles, fall below it in the middle, and rise again at the upper quantiles. Therefore, it's unlikely the residuals are normally distributed.  ##Shapiro-Wilk test  To truly determine that the residuals are normally distributed, I performed a Shapiro-Wilk test. While the Q-Q plots give a graphical representation of normality, the Shapiro-Wilk test - which returns a W-value between 0 and 1, with 1 meaning the sample is perfectly normal - gives a mathematical answer.  In addition to the W-value, the test returns a p-value. The null hypothesis of the test is that the data is normally distributed. Therefore, a p-value < 0.05 means the data is not normally distributed, as we reject the null hypothesis when p < 0.05.  The table below shows the results of the Shapiro-Wilk test.  | Model | W-value | p-value | | ----- | ------- | ------- | | SVM | 0.94 | < 0.01 | | RF | 0.88 | < 0.01 | | KNN | 0.90 | < 0.01 | | DNN | 0.97 | 0.04 |  The results confirm what we suspected from the Q-Q plot; no model returned normally distributed residuals. Though this is concerning, it doesn't invalidate the models.  ##Durbin-Watson test  The Durbin-Watson test examines whether the models' residuals have autocorrelation, meaning there is a self-repeating trned in the residuals. Essentially, if the models have significant autocorrelation, they're making the same mistake over and over. The test returns a value between 0 and 4, where a value of 2 indicates no autocorrelation. Values between 0 and 2 indicate positive autocorrelation, meaning if one point is higher than the previous, then the next point is likely to be even higher. Values between 2 and 4 indicate negative autocorrelation, meaning that the points are likely to have the opposite change from the previous points.  The table below shows the results of the Durbin-Watson test.  | Model | DW-value | | ----- | -------- | | SVM | 1.77 | | RF | 1.81 | | KNN | 1.85 | | DNN | 1.69 |  All the DW-values are between 1.5 and 2, indicating there's minimal autocorrelation.  ##Would we benefit from more data?  Currently, the data has two limits. First, it only goes back to the 1979-80 season so that it only includes players in the 3-point era. Second, it only includes players top-10 in MVP votes, so we could gain a few more datapoints by including everyone who received MVP votes.  I think the second limitation would hurt the models if removed, as it would further skew the distribution toward low vote shares. To examine if we'd benefit from more data, I plotted the learning curve for r-squared and mean squared error (MSE) for each model.  https://i.imgur.com/cCy7ZFe.png  https://i.imgur.com/N3Bnmch.png  The lines appear to get very close for the SVM and DNN, meaning more data is unlikely to help. However, for the KNN and RF, it's possible more data may increase the models' accuracy. Given the primary way to add data to this problem is to consider all players who received votes - which skews the data - adding more data probably won't help the models.  ***  # Results  The 4 graphs below show each model's results. Each player's current rank in NBA.com's MVP ladder. Note that these predictions were made using data from 1/14 before the games were played.  https://i.imgur.com/Vv1q7Fq.png  https://i.imgur.com/3L6rZ3j.png  https://i.imgur.com/xKj8G6e.png  https://i.imgur.com/KDPPNw7.png  All four models have Giannis as the winner, and Harden as the runner-up. Two models have Jokic coming in third, and the other two models (which have KD and Anthony Davis in third) have Jokic in fourth.  The graph below is the average predicted vote share of the four models for each player.  https://i.imgur.com/OsaEkpH.png  The average shows that the models have Giannis winning MVP by a decent margin. Harden also has a decent margin on Jokic.  ***  #Conclusion  The models crown Giannis as the MVP half-way through the year. Note that the models can't account for narrative, expectations, or voter fatigue; it's purely a statistical view.  This may change soon, as Harden scored 57 points the night I ran this analysis. With Capela out for the foreseeable and the Rockets still missing key pieces in Chris Paul and Eric Gordon, it's possible Harden ramps up his already absurd scoring pace, making a strong case for MVP. I'll run the models again with new data around the All-Star break to see if Harden caught up.  ***  This is my newest post with my open-source basketball analytics blog, [Dribble Analytics](http://dribbleanalytics.blogspot.com). The GitHub for this project is [here](https://github.com/dribbleanalytics/ml-mvp-predict).  You can check out the original piece [here](http://dribbleanalytics.blogspot.com/2019/01/ml-mvp-predict-midseason.html).  ***  **Tl;dr: table of results:**  |Rank|NBA.com MVP ladder|SVM|RF|KNN|DNN|**Average prediction**| --:|:--|:--|:--|:--|:--|:--| |1|Giannis|Giannis|Giannis|Giannis|Giannis|**Giannis**| |2|Harden|Harden|Harden|Harden|Harden|**Harden**| |3|Curry|Jokic|Davis|Jokic|KD|**Jokic**| |4|Kawhi|Kawhi|Jokic|Curry|Jokic|**Kawhi**| |5|Jokic|KD|Curry|Kawhi|Kawhi|**KD**| |6|PG13|Curry|Embiid|KD|PG13|**Curry**| |7|Embiid|Davis|PG13|Embiid|Embiid|**Davis**| |8|LeBron|PG13|Kawhi|LeBron|Davis|**Embiid**| |9|KD|Embiid|KD|PG13|Curry|**PG13**| |10|Davis|LeBron|LeBron|Davis|Lebron|**LeBron**|	reddit
12	"Hi! Three months ago, I posted online that most books on machine learning are too thick, which makes machine learning look very complex as engineering domain. I said that if I was to write a book on machine learning it would be a hundred-page book. That my post has become viral and I received two kinds of comments: 1) ""It's impossible: those books are so thick for a reason!"" and 2) ""Please write that book!""  So I wrote that book, called it ""The Hundred-Page Machine Learning Book"", designed and published it entirely myself (with the help of volunteers for copy editing) using Kindle Direct Publishing, put it entirely online on the ""read first, buy later"" principle, and now it's a huge success on Amazon.  Will be glad to answer your questions!  Proof: [https://twitter.com/burkov/status/1089895012488355842](https://twitter.com/burkov/status/1089895012488355842)  *****  OK, folks, the AMA is technically over. I will get back here from times to times during the day to see if there are some upvoted questions I didn't answer. Thank you, everyone, for your interest and great questions!  *****  OMG thank you Reddit for the GOLD! My first gold ever!"	reddit
13	"Prof. Dr. Max Welling is a research chair in Machine Learning at the University of Amsterdam and a VP Technologies at Qualcomm. He has a secondary appointment as a senior fellow at the Canadian Institute for Advanced Research (CIFAR). He is co-founder of ""Scyfer BV"" a university spin-off in deep learning which got acquired by Qualcomm in summer 2017. In the past he held postdoctoral positions at Caltech ('98-'00), UCL ('00-'01) and the U. Toronto ('01-'03). He received his PhD in '98 under supervision of Nobel laureate Prof. G. 't Hooft. Max Welling has served as associate editor in chief of IEEE TPAMI from 2011-2015 (impact factor 4.8). He serves on the board of the NIPS foundation since 2015 (the largest conference in machine learning) and has been program chair and general chair of NIPS in 2013 and 2014 respectively. He was also program chair of AISTATS in 2009 and ECCV in 2016 and general chair of MIDL 2018. He has served on the editorial boards of JMLR and JML and was an associate editor for Neurocomputing, JCGS and TPAMI. He received multiple grants from Google, Facebook, Yahoo, NSF, NIH, NWO and ONR-MURI among which an NSF career grant in 2005. He is recipient of the ECCV Koenderink Prize in 2010. Welling is in the board of the Data Science Research Center in Amsterdam, he directs the Amsterdam Machine Learning Lab (AMLAB), and co-directs the Qualcomm-UvA deep learning lab (QUVA) and the Bosch-UvA Deep Learning lab (DELTA).   He will be with us at 12:30 ET (ET, 17:30 UT) to answer your questions!         "	reddit
14	"The **results** of ML algorithms and software are really cool. But the actual work itself is nowhere near exciting as I thought it would be. I've completely shifted my focus from ML/AI to Data Infrastructure and although the latter is less flashy, the work is also much more fun.     From my experience, a lot of ML work was about 75% Data Curation, about 5% building pipelines and designing systems, and about 20% tuning parameters to get better results. Imagine someone gave you a massive 10 GB excel sheet, and your job is to use the data to predict sales; the vast majority of your work is going to be trimming the data and documenting it, not actually building the model.     Obviously this is only based on my opinion (you might have a much different experience). But as someone who has worked in multiple subfields including ML, infrastructure, embedded, I can very honestly say ML was my least favorite, while infrastructure was the most fun. The whole point of data infrastructure is to build systems, classes, and pipelines to maximize efficiency... so you're actually engineering things the whole day at work.     But if you want a cool job to brag about at parties, then ""I work on artificial intelligence"" is basically unbeatable.  Edit : Clearly this is a popular opinion"	reddit
15	Source: https://www.facebook.com/share/v/1DN6mxTykr/	reddit
16	I	reddit
17	"I‚Äôm serious... when I see a kyrie quote without an accompanying video, I‚Äôm always suspicious it‚Äôs actually something written by a machine learning artificial intelligence.   For fun I went to https://talktotransformer.com/ and gave it the starting prompt **‚Äùbasketball is about more than winning games...‚Äù**  Here‚Äôs what the website spat out.  Tell me this doesn‚Äôt sound like something Kyrie would say:  > Basketball is about more than winning games. It's also about achieving an ultimate and lasting legacy. It's all about brothers, cousins, teachers and coaches who helped mold these seven young men into the men they are today, men who aspire to be the best in the world. No two young men with similar backgrounds, goals and outlooks have ever come together on the basketball court. When you win games and get near the Finals, you win an organization, you win a loyalty, you win a relationship with fans and you're seeing more players who are going to become all-stars of this game. So I think winning plays a major role.  Here‚Äôs another one:  > **I don‚Äôt worry about what the basketball fans think.** I understand when I play the way I do. I know I need to put more out there to make a game, and I feel great. I don't think I feel any less than I do at any point in my career, so if you don't know that, I think I just gave the basketball fans an indication. It's nothing more than that. I'm not looking for my name to be out there. I know I can play, and I'm looking forward to the next.  What matters is how it is for those outside the sport and how they feel. If you‚Äôre going to work for this organization, you‚Äôre going to feel the pain of those who go there. We care about that. Those are the things that you‚Äôre going to hear a lot from us. That is why I believe in this organization and that is why I believe in myself as an athlete.  --- To prove my point further, I took some of that machine-generated text, had Watson's text-to-speech read it for me, used then used a rough deepfake lipsync tech to generate **this video: https://youtu.be/E6MZrOd7zbg**   Almost no part of this is human Kyrie.  Granted, this is extremely rough and done in minutes, but imagine someone with more skill?   **Are we even sure Kyrie Irving is a real person?**  ---  FWIW, one of these quotes is allegedly real and the other is something I machine generated:  **Quote 1:**  ""*At the end of the day it's an entertainment league.*  It's a circus, the fans are rabid and it's unpredictable.  For most of the people who get into it and follow it closely, it's a fun show to be a part of.  What's the point of winning if you can't have fun with it? If we go all out, and if we do something here and there, it will be fun. There are ups and downs, there are bad days, and there are good days. We just need to stay consistent. I just think that's the only way to go.""  **Quote 2:**  ""*At the end of the day, it's an entertainment league.*  We're very drama filled.  Everything regurgitates on all these media platforms which is part of our society.  I can't do anything about it except be a pillar in our locker room, be very communicative, and when I'm out there with the guys just impact winning.  ...  Good luck figuring out which is which.  **TL;DR: [We are reaching a point (or have already reached it) where Kyrie Irving is indistinguishable from a machine learning artificial intelligence](https://youtu.be/E6MZrOd7zbg)**"	reddit
18	"**Tl;dr: Average predicted DBPM for [guards](https://i.imgur.com/PP9Uubn.png), [forwards](https://i.imgur.com/m1YR2l5.png), and [centers](https://i.imgur.com/OFQiKuv.png).**  This post has a lot of graphs. If you don't want to click on each one individually, they're all in an imgur album [here](https://imgur.com/a/ExxR2VS).  ***  # Introduction  College defense differs from NBA defense significantly. Aside from NBA players being significantly more talented offensively than an average college player, college players play in vastly different defensive schemes. For example, many colleges use a zone for the whole game. This forces the defenders into unique roles that might not translate to the NBA.  For example, in a zone, a center roams the paint. Therefore, a college center can in theory go many games solely as a rim protector. This leads to the player having inflated defensive stats, as they will naturally get most of the team's blocks if they are always in the paint. While rim protection is an important part of a big man's defense, one-on-one defense is also important. However, in a zone, the center probably has very few opportunities to play one-on-one.  Because of the different nature of college and NBA defense, it is difficult to predict how a college player's defense will translate to the NBA. Even for players who not only play on the top teams but also rarely play zone, projecting defense is a difficult task. This is seen through Karl-Anthony Towns' defense in college compared to his defense in the NBA.  In college, Towns was a star defender. Many scouting reports cited his defense as a big positive. Towns had a stellar 10.8 DBPM and a block percentage of 11.5%. These factors coupled with the fact that Towns played for Kentucky - a team that rarely ran zone, and also had a high strength of schedule - made many think his defense would be a sure thing.  However, Towns now struggles on defense. His career average DBPM in the NBA is only 0.8, which is very low for a center.  Towns is one of many examples of players whose defense in college fails to translate to the NBA. Though defense can't truly be measured by blocks, steals, or other metrics, we can try to use all of these metrics together to predict a player's NBA defense. No defensive metric perfectly captures a player's defensive ability. However, DBPM gives a general sense of a player's defense. We will use several college statistics to try to predict the upcoming draft class's DBPM, thereby showing which players in the draft class will be the best defenders.  ***  #Methods  First, I made a dataset of all college players selected in the first round since the 2011 draft. I recorded the following stats for them:  | College stats | NBA stats | Other information | | ------------- | ------------- | ------------- | | Seasons in college | G | Wingspan (in) | | G | MPG | Height (in) | | MPG  | STL/G | Position | | STL/G  | BLK/G | | BLK/G  | DWS | | PF/G  | DWS/48 | | STL% | DBPM | | BLK% | | | DWS | | | DWS/40 | | | DBPM | |  I also restricted the players in the 2017 draft to those who played more than 10 games so that their DBPM is not completely unrealistic. 171 players met these restrictions.  Then, I recorded the above stats for the top 35 picks in [The Ringer's 2018 NBA Draft Guide](http://nbadraft.theringer.com) (as of 6/13 update).  Using the current players dataset, I created 3 different machine learning models for guards, forwards, and centers:  1. Linear regression 2. Ridge regression 3. Lasso regression  The model took most of the aforementioned college stats and other information as inputs, and output a player's NBA DBPM. Though DBPM is not a perfect defensive stat, it gives us a good idea of whether a player is a good or bad defender.  I created each of the three models with varying test sizes. After the models for each position were created, I put in the 2018 draft class's data to predict their DBPM. Players such as Marvin Bagley who The Ringer described as a ""big"" were put in both the centers' model and the forwards' model, as these players will likely play both forward and center.  ***  #Comparing college and NBA stats  On its own, college DBPM is not a bad predictor of NBA DBPM. The graph below shows the correlation between college and NBA DBPM for all the 171 players in our dataset.  https://i.imgur.com/54s1snh.png  The regression is statistically significant, as its p-value is less than 0.05. The rsquared of 0.46 is not bad, and the correlation coefficient (r) of 0.678 shows that the correlation between college and NBA DBPM is not too weak.  The regression is also statistically significant when it is split up into guards, forwards, and centers:  https://i.imgur.com/45BQaTX.png  https://i.imgur.com/1JCZSQD.png  https://i.imgur.com/CB0lmPX.png  Although all these regressions have statistical significance, their correlation coefficient is not very high. Also, these only consider one factor (college DBPM) in predicting a player's NBA DBPM; this regression would predict players who were solely rim protectors in limited minutes in college (and therefore had a high DBPM) as some of the best defenders in the league.  Therefore, to make the prediction more accurate, we must consider other factors as well. Before we add other stats such as steals and blocks into the model, we should test the similarity between blocks and steals for our current players dataset in college and our draft class dataset. If the distribution, mean, and standard deviation for the current players and draft class histograms seem close, then the comparison is fair.  First, let's look at the steals histograms:  https://i.imgur.com/ycNNTYn.png  https://i.imgur.com/zYfWVFD.png  Though the draft class's average steals is slightly lower than the sample's average steals, they are close enough such that a comparison is fair. Furthermore, this draft class has a lot of big men (who don't get many steals) compared to other classes, so it's expected that the average steals is lower. The standard deviation of steals is lower for the draft class's dataset, demonstrating that there is less variance in their steals.  Let's look at the same thing for blocks:  https://i.imgur.com/95155mi.png  https://i.imgur.com/TlMIvRF.png  As expected, the draft class has a higher average of blocks, as there are more centers than usual. The higher average also lends to a higher standard deviation, as there will be more variance in blocks if the draft class is mostly centers and guards (because the difference in their blocks will be large). Nevertheless, the comparison is fair.  ***  #Model results: guards  For guards, 20% of the current players dataset was used as the test set, and 80% was used as the training set.  ##Regression accuracy  The two graphs below show the accuracy of the three regressions. The most accurate regression will have the highest rsquared and the lowest mean squared error.  https://i.imgur.com/RAkFtw7.png  https://i.imgur.com/7v0grVm.png  The rsquared and mean squared error shows that the linear regression is the least accurate regression, followed by the ridge regression. The lasso regression, then, is the most accurate regression.  This is further shown through the regressions' cross-validation scores for explained variance. The linear regression's cross-validation score was -1.06. The 95% confidence interval was +/- 3.84. So, the bad cross-validation score for the linear regressions shows that the regression is overfitting.  Meanwhile, the ridge and lasso regressions have more reasonable cross-validation scores. The ridge regression has the best cross-validation score at 0.78, with a 95% confidence interval of +/- 0.20. The lasso regression - the most accurate regression according to rsquared and mean squared error - had a worse cross-validation score; its score was 0.72, with a 95% confidence interval of +/- 0.33  Therefore, the linear regression is the least accurate when we consider cross-validation scores. However, the better cross-validation score for the ridge regression shows that the ridge regression might be more accurate than the lasso regression. For the purpose of this analysis, we'll say that the ridge and lasso regressions are both equally accurate.  ##**Regression results**  The following three graphs show the predicted DBPM for the draft class's guards by the linear regression, ridge regression, and lasso regression, respectively.  https://i.imgur.com/8fbCbYZ.png  https://i.imgur.com/AVqzGR8.png  https://i.imgur.com/kQcmwgh.png  The linear regression, which we determined was the least accurate of the three, showed that De'Anthony Melton, Khyri Thomas, and Josh Okogie will be the only positive defenders from the guards of this draft class. The other two regressions - which were more accurate than the linear regression - found that De'Anthony Melton will be the only guard with a positive defensive impact. All three models predict a DBPM for Melton of over 1, which is phenomenal for a guard. For comparison, Marcus Smart has a career DBPM of 1.1.  The graph below averages the predicted DBPM from all three models.  https://i.imgur.com/PP9Uubn.png  On average between the three regressions, De'Anthony Melton is the only guard who the models project to be a positive defender. Collin Sexton's predicted DBPM of less than -2.0 puts him at a level of defense worse than Damian Lillard (whose career DBPM is -1.5), yet not as bad as Isaiah Thomas (whose career DBPM is -2.8).  ***  #Model results: forwards  Initially, I expected to do make the model using all forwards. However, the forwards model had a very low rsquared (all three models had an rsquared below 0.375) and a negative cross-validation score. So, I split it up into small forwards and power forwards.   It ended up that the power forwards regression was even worse than the forwards regression; all three models had a negative rsquared, which means the models were less accurate than us saying ""every single forward will have the same DBPM of x.""  Because the small forwards model was much more accurate than the model for all forwards, we put the players into the small forwards model. The small forwards model's test set was 25% of the dataset.  ##Regression accuracy  The two graphs below show the accuracy of the three regressions.  https://i.imgur.com/lVMhVj4.png  https://i.imgur.com/fNgSkOf.png  The linear regression was the least accurate model. The ridge and lasso regressions had almost exactly the same rsquared (0.790 for the ridge regression vs. 0.792 for the lasso regression), so they were similarly more accurate than the linear model.   This accuracy is further seen through the cross-validation score. The linear regression had the lowest cross-validation score (0.27 with a 95% confidence interval of +/- 1.28). The lasso regression's cross-validation score was 0.46 with a 95% confidence interval of +/- 0.88. The ridge regression had the highest cross-validation score (0.62 with a 95% confidence interval of +/- 0.69).  The cross-validation scores help us come to the conclusion that the ridge regression is the most accurate model, followed by the lasso regression, and then the linear regression.  ##**Regression results**  It's important to remember that we are putting in the draft class's forwards (which includes players like Jaren Jackson Jr. and Marvin Bagley, who are a PF/C) into a model built of small forwards. Though this might seem odd, as some of these players will never play small forward, I think of it in a way that the model essentially outputs the big man's ability as a perimeter defender. For example, if Jaren Jackson Jr. guarded the same types of players as a SF, his DBPM would be x.  The three graphs below show the projected DBPM for the draft class's forwards using our SF model with a linear regression, ridge regression, and lasso regression, respectively.  https://i.imgur.com/OhNcvZy.png  https://i.imgur.com/GhpFQZ7.png  https://i.imgur.com/HevOUDN.png  Interestingly, Jaren Jackson Jr. is the best defender in all three models, even though he is a big man. This shows how great his defense is; the models project him to be a better perimeter defender as a big man than other actual perimeter players like Miles Bridges.  Though Kevin Knox is projected as the worst defender in all three models, his projected DBPM is not on the same level as the worst forward defenders in the league. For example, while Knox's predicted DBPM is less than -0.5, Melo and Andrew Wiggins have career DBPMs of -1.2 and -2.1, respectively.  The graph below shows the average predicted DBPM for forwards.  https://i.imgur.com/m1YR2l5.png  ***  #Model results: centers  For centers, 10% of the current players dataset was used as the test set. Though in most cases this is too low and would lead to overfitting, only 30 players were in the centers dataset, so using 20% or more of the dataset would lead to inaccurate results. Furthermore, the overfitting problem can be examined using cross-validation.  ##Regression accuracy  The two graphs below show the accuracy of the three regressions.  https://i.imgur.com/7uWkafN.png  https://i.imgur.com/ZPRvD1M.png  All three regressions had almost identical rsquareds; the linear regression had an rsquared of 0.742, while the ridge and lasso regressions both had an rsquared of 0.741. All three models also had very similar mean squared errors; though the linear regression had the lowest mean squared error, it was only 0.004 less than the ridge regression, which had the highest mean squared error.  Without considering cross-validation, all three models are similarly accurate. The linear regression has a cross-validation score of 0.55 with a 95% confidence interval of +/- 0.90. The ridge regression has the exact same score and confidence interval as the linear regression. Meanwhile, the lasso regression's score is slightly better at 0.60, with a 95% confidence interval of +/- 0.79. Therefore, we can say the lasso regression is slightly more accurate than the linear and ridge regressions, though all three are pretty close in terms.  ##**Regression results**  The three graphs below show the predicted DBPM for the draft class's centers using a linear regression, ridge regression, and lasso regression, respectively.  https://i.imgur.com/Z7drrgs.png  https://i.imgur.com/MFqIpZM.png  https://i.imgur.com/RdmGaps.png  All three models project that none of the centers will be net negative defenders. However, DBPM greatly favors centers; even Jahlil Okafor, one of the worst defensive centers in the NBA, has a career DBPM of only -0.8. Karl-Anthony Towns, who gives little effort on defense and is also criticized as an underwhelming defender given his physical tools, has a career DBPM of 0.8.  Therefore, even though all three models predict Deandre Ayton will be a positive defender, the models predict he will be a worse defender than Towns.  All three models confirm the view that Mo Bamba is the best defensive prospect in this class. Surprisingly, Robert Williams has a very similar predicted DBPM to Jaren Jackson Jr., the prospect who many view as the best defender in the draft after Bamba.  The graph below shows the average of the predicted DBPM from the three models.  https://i.imgur.com/OFQiKuv.png  ***  #Conclusion  The models predict the De'Anthony Melton will be the best guard defender by a large margin, and Collin Sexton will be the worst by a decent margin.   Out of the true small forwards, the models predict that Zhaire Smith will be the best defender. However, Jaren Jackson Jr. has a higher predicted DBPM than him in the same model, demonstrating Jackson's huge potential as a perimeter defender.   Though it predicts Deandre Ayton will be a positive defender, the model says Ayton will be a worse defender than Towns. Meanwhile, the model predicts Bamba will be the best big man defender in the draft.  ***  This is my third post with my new open-source basketball analytics blog, [Dribble Analytics](http://dribbleanalytics.blogspot.com). This is also my second machine learning project. The GitHub for this project is [here](https://github.com/dribbleanalytics/draft-class-defenders-ml).  You can check out the original piece [here](https://dribbleanalytics.blogspot.com/2018/06/using-machine-learning-to-predict-best.html )."	reddit
19	First, read fucking Hastie, Tibshirani, and whoever. Chapters 1-4 and 7-8. If you don't understand it, keep reading it until you do.   You can read the rest of the book if you want. You probably should, but I'll assume you know all of it.   Take Andrew Ng's Coursera. Do all the exercises in python and R. Make sure you get the same answers with all of them.   Now forget all of that and read the deep learning book. Put tensorflow and pytorch on a Linux box and run examples until you get it. Do stuff with CNNs and RNNs and just feed forward NNs.  Once you do all of that, go on arXiv and read the most recent useful papers. The literature changes every few months, so keep up.   There. Now you can probably be hired most places. If you need resume filler, so some Kaggle competitions. If you have debugging questions, use StackOverflow. If you have math questions, read more. If you have life questions, I have no idea.	reddit
20	"Prove me wrong...   I assert all of Artificial Intelligence and what it is, is just ""Machine Learning"", regardless of program.   Any piece of software that is built to be autonomous and to LEARN from input and output, or to search for it, is MACHINE LEARNING.   The rest is purpose lol.  "	reddit
21	"I've been in this career for 6+ years and I can count on one hand the number of times that I have seriously considered building a machine learning model as a potential solution. And I'm far from the only one with a similar experience.  Most ""data science"" problems don't require machine learning.  Yet, there is SO MUCH content out there making students believe that they need to focus heavily on building their Machine Learning skills.  When instead, they should focus more on building a strong foundation in statistics and probability (making inferences, designing experiments, etc..)  If you are passionate about building and tuning machine learning models and want to do that for a living, then become a Machine Learning Engineer (or AI Engineer)  Otherwise, make sure the Data Science jobs you are applying for explicitly state their need for building predictive models or similar, that way you avoid going in with unrealistic expectations."	reddit
22	Cutting-plane methods are well-studied localization(and optimization) algorithms. We show that they provide a natural framework to perform machinelearning ---and not just to solve optimization problems posed by machinelearning--- in addition to their intended optimization use. In particular, theyallow one to learn sparse classifiers and provide good compression schemes.Moreover, we show that very little effort is required to turn them intoeffective active learning methods. This last property provides a generic way todesign a whole family of active learning algorithms from existing passivemethods. We present numerical simulations testifying of the relevance ofcutting-plane methods for passive and active learning tasks.	arxiv
23	The Internet as we know it Today, comprises several fundamental interrelated networks, among which is the Internet of Things (IoT). Despite their versatility, several IoT devices are vulnerable from a security perspective, which renders them as a favorable target for multiple security breaches, especially botnet attacks. In this study, the conceptual frameworks of IoT botnet attacks will be explored, alongside several machinelearning based botnet detection techniques. This study also analyzes and contrasts several botnet Detection techniques based on the Bot-IoT Dataset; a recent realistic IoT dataset that comprises state-of-the-art IoT botnet attack scenarios.	arxiv
24	Disruption prediction and mitigation is of key importance in the development of sustainable tokamakreactors. Machine learning has become a key tool in this endeavour. In this paper multiple machinelearning models will be tested and compared. A particular focus has been placed on their portability.This describes how easily the models can be used with data from new devices. The methods used inthis paper are support vector machine, 2-tiered support vector machine, random forest, gradient boostedtrees and long-short term memory. The results show that the support vector machine performanceis marginally better among the standard models, while the gradient boosted trees performed the worst.The portable variant of each model had lower performance. Random forest obtained the highest portableperformance. Results also suggest that disruptions can be detected as early as 600ms before the event.An analysis of the computational cost showed all models run in less than 1ms, allowing sufficient timefor disruption mitigation.	arxiv
25	Regression problems have been widely studied in machinelearning literature resulting in a plethora of regression models and performance measures. However, there are few techniques specially dedicated to solve the problem of how to incorporate categorical features to regression problems. Usually, categorical feature encoders are general enough to cover both classification and regression problems. This lack of specificity results in underperforming regression models. In this paper,we provide an in-depth analysis of how to tackle high cardinality categor-ical features with the quantile. Our proposal outperforms state-of-the-encoders, including the traditional statistical mean target encoder, when considering the Mean Absolute Error, especially in the presence of long-tailed or skewed distributions. Besides, to deal with possible overfitting when there are categories with small support, our encoder benefits from additive smoothing. Finally, we describe how to expand the encoded values by creating a set of features with different quantiles. This expanded encoder provides a more informative output about the categorical feature in question, further boosting the performance of the regression model.	arxiv
26	Quantum phase estimation is a paradigmatic problem in quantum sensing andmetrology. Here we show that adaptive methods based on classical machinelearning algorithms can be used to enhance the precision of quantum phase estimation when noisy non-entangled qubits are used as sensors. We employ the Differential Evolution (DE) and Particle Swarm Optimization (PSO) algorithms to this task and we identify the optimal feedback policies which minimize the Holevo variance. We benchmark these schemes with respect to scenarios that include Gaussian and Random Telegraph fluctuations as well as reduced Ramsey-fringe visibility due to decoherence. We discuss their robustness against noise in connection with real experimental setups such as Mach-Zehnder interferometry with optical photons and Ramsey interferometry in trapped ions,superconducting qubits and nitrogen-vacancy (NV) centers in diamond.	arxiv
27	"The fifth generation (5G) and beyond wireless networks are foreseen to operate in a fully automated manner, in order to fulfill the promise of ultra-short latency, meet the exponentially increasing resource requirements, and offer the quality of experience (QoE) expected from end-users. Among the ingredients involved in such environments, network slicing enables the creation of logical networks tailored to support specific application demands (i.e., service level agreement SLA, quality of service QoS, etc.) on top of physical infrastructure. This creates the need for mechanisms that can collect spatiotemporal information on users'service consumption, and identify meaningful insights and patterns, leveraging machinelearning techniques. In this vein, our paper proposes a framework dubbed""SOCLfor"" the Service Oriented CLustering, analysis and profiling of users (i.e., humans, sensors, etc.) when consuming enhanced Mobile BroadBand (eMBB) applications, internet of things (IoT) services, and unmanned aerial vehicles services (UAVs). SOCL relies mainly on the realistic network simulation framework""network slice planne""(NSP), and two clustering methods namely K-means and hierarchical clustering. The obtained results showcase interesting features, highlighting the benefit of the proposed framework."	arxiv
28	Artificial intelligence (AI) enabled radiomics has evolved immensely especially in the field of oncology. Radiomics provide assistancein diagnosis of cancer, planning of treatment strategy, and predictionof survival. Radiomics in neuro-oncology has progressed significantly inthe recent past. Deep learning has outperformed conventional machinelearning methods in most image-based applications. Convolutional neu-ral networks (CNNs) have seen some popularity in radiomics, since theydo not require hand-crafted features and can automatically extract fea-tures during the learning process. In this regard, it is observed that CNNbased radiomics could provide state-of-the-art results in neuro-oncology,similar to the recent success of such methods in a wide spectrum ofmedical image analysis applications. Herein we present a review of the most recent best practices and establish the future trends for AI enabled radiomics in neuro-oncology.	arxiv
29	The diagnosis, prognosis, and treatment of patients with musculoskeletal (MSK) disorders require radiology imaging (using computed tomography, magnetic resonance imaging(MRI), and ultrasound) and their precise analysis by expert radiologists. Radiology scans can also help assessment of metabolic health, aging, and diabetes. This study presents how machinelearning, specifically deep learning methods, can be used for rapidand accurate image analysis of MRI scans, an unmet clinicalneed in MSK radiology. As a challenging example, we focus on automatic analysis of knee images from MRI scans and study machine learning classification of various abnormalities including meniscus and anterior cruciate ligament tears. Using widely used convolutional neural network (CNN) based architectures, we comparatively evaluated the knee abnormality classification performances of different neural network architectures under limited imaging data regime and compared single and multi-view imaging when classifying the abnormalities. Promising results indicated the potential use of multi-view deep learning based classification of MSK abnormalities in routine clinical assessment.	arxiv
30	In this technical report, we present our solution of KDD Cup 2021 OGB Large-Scale Challenge - PCQM4M-LSC Track. We adopt Graphormer and ExpC as our basic models. We train each model by 8-fold cross-validation, and additionally train two Graphormer models on the union of training and validation sets with different random seeds. For final submission, we use a naive ensemble for these 18 models by taking average of their outputs. Using our method, our team MachineLearning achieved 0.1200 MAE on test set, which won the first place in KDD Cup graph prediction track.	arxiv
31	High temperature properties of ceria surfaces are important for many applications. Here we report the temperature dependences of surface energy for the (111) and (110) CeO2 obtained in the framework of the extended two-stage upsampled thermodynamic integration using Langevin dynamics (TU-TILD). The method was used together with machinelearning potentials called moment tensor potentials (MTPs), which were fitted to the results of the ab initio MD calculations for (111) and (110) CeO2 at different temperatures. The parameters of MTPs training and fitting were tested and the optimal algorithm for the ceria systems was proposed. We found that the temperature increases from 0 K to 2100 K led to the decrease of the Helmholtz free energy of (111) CeO2 from 0.78 J/m2 to 0.64 J/m2. The energy of (110) CeO2 dropped from 1.19 J/m2 at 0 K to 0.92 J/m2 at 1800 K. We show that it is important to take anharmonicity into account as simple consideration of volume expansion gives wrong temperature dependences of the surface energies.	arxiv
