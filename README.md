# Projets en Programmation de Sp√©cialit√© : Python
üë®‚Äçüíª R√©alis√© par Khadim & Saliou ‚Äì Master 1 Informatique, Lyon 2

Dans le cadre de ce projet, j‚Äôai d√©velopp√© un programme en Python en suivant l‚Äôensemble des √©tapes du cycle de vie logiciel : la sp√©cification, l‚Äôanalyse, la conception, le codage, la v√©rification et la maintenance. Ce travail m‚Äôa permis de mettre en pratique mes comp√©tences en programmation et de d√©montrer ma capacit√© √† concevoir une application compl√®te, bien structur√©e et √©volutive.
Le projet s‚Äôappuie sur les TD 3 √† 10, dont les deux derniers offrent davantage de libert√© dans la conception. Le rendu se fait en trois versions successives, chacune repr√©sentant une √©tape d‚Äô√©volution et d‚Äôam√©lioration du projet.
## TD 3 : acquisition de donn√©es (version 1)
### Objectifs:
#### Partie 1 ‚Äî Collecte des donn√©es
**But** : extraire des documents textuels √† partir de sources externes (APIs).

1.1 Interroger Reddit avec la librairie praw pour r√©cup√©rer le champ textuel selftext.

1.2 Interroger Arxiv avec urllib et parser les r√©sultats XML avec xmltodict.

1.3 Nettoyer les textes (supprimer les \n).

1.4 Alimenter une liste Python docs contenant uniquement le contenu textuel des documents.
#### Partie 2 ‚Äî Construction et sauvegarde du corpus
**But** : √©viter de r√©interroger les APIs √† chaque ex√©cution.

2.1 Cr√©er un DataFrame pandas avec trois colonnes :

id ‚Üí identifiant unique du document

texte ‚Üí contenu textuel du document

source ‚Üí origine du texte (reddit ou arxiv)

2.2 Sauvegarder ce tableau sur disque au format .csv avec le s√©parateur de tabulation \t.

2.3 Ajouter du code permettant de recharger directement ce fichier lors d‚Äôune prochaine ex√©cution, sans repasser par les appels API
#### Partie 3 ‚Äî Premi√®res manipulations du corpus
**But** : explorer et pr√©parer les donn√©es textuelles.

3.1 Afficher la taille du corpus (nombre de documents).

3.2 Calculer, pour chaque document, le nombre de mots et de phrases (avec split(" ") et split(".")).

3.3 Supprimer les documents trop courts (< 20 caract√®res).

3.4 Fusionner tous les textes en une seule cha√Æne de caract√®res (" ".join(...)) pour une analyse globale.

---
## Pr√©requis
- Python 3.10 ou sup√©rieur

## Installation
Avant de lancer le projet, installe les d√©pendances :

```bash
pip install -r requirements.txt
```
## strucure du Projet
```bash
python_project/
‚îú‚îÄ‚îÄ td3/
‚îÇ   ‚îú‚îÄ‚îÄ reddit_arxiv.py # recup√©ration des textes
‚îÇ   ‚îú‚îÄ‚îÄ corpus.py # construction et chargement du corpus
‚îÇ   ‚îú‚îÄ‚îÄ analyze.py # analyse du corpus
‚îú‚îÄ‚îÄ main.py # fichier principale
‚îú‚îÄ‚îÄ data/  # corpus sauvegard√©
‚îú‚îÄ‚îÄ README.md  # ce fichier
‚îú‚îÄ‚îÄ requirements.txt # librairies necesssaires
```
## Version

- Version **v1** : comprend TD3, TD4 et TD5

